{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOdJP8qAyLqz3/KXAO2MWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x1096023/BCI_final_project/blob/main/CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw_XBG1"
      ],
      "metadata": {
        "id": "NAFQqw_S1E81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'ds003478'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if (file[4:7] != '038') :\n",
        "                if file[22:24] == '01':\n",
        "                    set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0,0,0,0,0,0,0,0,0,0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data and common channels\n",
        "all_data = []\n",
        "common_channels = None\n",
        "\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    raw.resample(100)  # Downsample to 100 Hz\n",
        "    all_data.append((raw.get_data(), raw.info['ch_names'], raw.info['sfreq']))\n",
        "\n",
        "    # Determine the common channels\n",
        "    if common_channels is None:\n",
        "        common_channels = set(raw.info['ch_names'])\n",
        "    else:\n",
        "        common_channels.intersection_update(raw.info['ch_names'])\n",
        "\n",
        "# Convert common channels to a list\n",
        "common_channels = list(common_channels)\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data, _, _ in all_data])\n",
        "\n",
        "for data_tuple, label in zip(all_data, labels):\n",
        "    data, ch_names, sfreq = data_tuple\n",
        "\n",
        "    # Select only common channels\n",
        "    ch_indices = [ch_names.index(ch) for ch in common_channels]\n",
        "    data = data[ch_indices, :]\n",
        "\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the maximum length, pad the data with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    elif data.shape[1] > max_length:\n",
        "        # If data length is greater than the maximum length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "    else:\n",
        "        padded_data = data\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and labels to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=100)\n",
        "\n",
        "# Convert to DMatrix format, which is the internal data format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',  # Binary classification\n",
        "    'eval_metric': 'logloss',  # Evaluation metric\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = bst.predict(dtest)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_i0aIaWE1UPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw_XBG2"
      ],
      "metadata": {
        "id": "CDybCc8R1hUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'ds003478'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if (file[4:7] != '038') :\n",
        "                if file[22:24] == '02':\n",
        "                    set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0,0,0,0,0,0,0,0,0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data and common channels\n",
        "all_data = []\n",
        "common_channels = None\n",
        "\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    raw.resample(100)  # Downsample to 100 Hz\n",
        "    all_data.append((raw.get_data(), raw.info['ch_names'], raw.info['sfreq']))\n",
        "\n",
        "    # Determine the common channels\n",
        "    if common_channels is None:\n",
        "        common_channels = set(raw.info['ch_names'])\n",
        "    else:\n",
        "        common_channels.intersection_update(raw.info['ch_names'])\n",
        "\n",
        "# Convert common channels to a list\n",
        "common_channels = list(common_channels)\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data, _, _ in all_data])\n",
        "\n",
        "for data_tuple, label in zip(all_data, labels):\n",
        "    data, ch_names, sfreq = data_tuple\n",
        "\n",
        "    # Select only common channels\n",
        "    ch_indices = [ch_names.index(ch) for ch in common_channels]\n",
        "    data = data[ch_indices, :]\n",
        "\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the maximum length, pad the data with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    elif data.shape[1] > max_length:\n",
        "        # If data length is greater than the maximum length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "    else:\n",
        "        padded_data = data\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and labels to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=100)\n",
        "\n",
        "# Convert to DMatrix format, which is the internal data format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',  # Binary classification\n",
        "    'eval_metric': 'logloss',  # Evaluation metric\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = bst.predict(dtest)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OywdOGv11kah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw_forest1"
      ],
      "metadata": {
        "id": "g86bE--O1-mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'ds003478'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if (file[4:7] != '038') :\n",
        "                if file[22:24] == '01':\n",
        "                    set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0,0,0,0,0,0,0,0,0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data and common channels\n",
        "all_data = []\n",
        "common_channels = None\n",
        "\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    raw.resample(100)  # Downsample to 100 Hz\n",
        "    all_data.append((raw.get_data(), raw.info['ch_names'], raw.info['sfreq']))\n",
        "\n",
        "    # Determine the common channels\n",
        "    if common_channels is None:\n",
        "        common_channels = set(raw.info['ch_names'])\n",
        "    else:\n",
        "        common_channels.intersection_update(raw.info['ch_names'])\n",
        "\n",
        "# Convert common channels to a list\n",
        "common_channels = list(common_channels)\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data, _, _ in all_data])\n",
        "\n",
        "for data_tuple, label in zip(all_data, labels):\n",
        "    data, ch_names, sfreq = data_tuple\n",
        "\n",
        "    # Select only common channels\n",
        "    ch_indices = [ch_names.index(ch) for ch in common_channels]\n",
        "    data = data[ch_indices, :]\n",
        "\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the maximum length, pad the data with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    elif data.shape[1] > max_length:\n",
        "        # If data length is greater than the maximum length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "    else:\n",
        "        padded_data = data\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and labels to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=100)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Cu1mu9D2BTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw_forest2"
      ],
      "metadata": {
        "id": "x29SVEDE2D4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'ds003478'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if (file[4:7] != '038') :\n",
        "                if file[22:24] == '02':\n",
        "                    set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0,0,0,0,0,0,0,0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data and common channels\n",
        "all_data = []\n",
        "common_channels = None\n",
        "\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    raw.resample(100)  # Downsample to 100 Hz\n",
        "    all_data.append((raw.get_data(), raw.info['ch_names'], raw.info['sfreq']))\n",
        "\n",
        "    # Determine the common channels\n",
        "    if common_channels is None:\n",
        "        common_channels = set(raw.info['ch_names'])\n",
        "    else:\n",
        "        common_channels.intersection_update(raw.info['ch_names'])\n",
        "\n",
        "# Convert common channels to a list\n",
        "common_channels = list(common_channels)\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data, _, _ in all_data])\n",
        "\n",
        "for data_tuple, label in zip(all_data, labels):\n",
        "    data, ch_names, sfreq = data_tuple\n",
        "\n",
        "    # Select only common channels\n",
        "    ch_indices = [ch_names.index(ch) for ch in common_channels]\n",
        "    data = data[ch_indices, :]\n",
        "\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the maximum length, pad the data with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    elif data.shape[1] > max_length:\n",
        "        # If data length is greater than the maximum length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "    else:\n",
        "        padded_data = data\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and labels to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=100)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IvoDcZgX2HAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB1"
      ],
      "metadata": {
        "id": "-Et_DEGu2iG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'trans-01'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if file[0:3] != '052':\n",
        "                set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,\n",
        "0 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,1 ,0 ,0 ,1,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1,1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data\n",
        "all_data = []\n",
        "\n",
        "# Frequency bands\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    all_data.append(raw.get_data())\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data in all_data])\n",
        "\n",
        "# Truncate or pad data to match the maximum data length\n",
        "for data, label in zip(all_data, labels):\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the max length, pad with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    else:\n",
        "        # If data length is greater than or equal to the max length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    # Add to feature list\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and target to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=150)\n",
        "\n",
        "# Convert to DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set XGBoost parameters\n",
        "params = {\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'binary:logistic',  # Binary classification\n",
        "    'eval_metric': 'logloss',  # Evaluation metric\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = bst.predict(dtest)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kiyHerbA2lyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB2"
      ],
      "metadata": {
        "id": "gUnfUErn2o_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import asrpy\n",
        "import mne_icalabel\n",
        "\n",
        "# 設置資料夾路徑\n",
        "folder_path = r'trans-02'\n",
        "set_files = []\n",
        "# 列出資料夾中的所有 .set 文件\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if(file[0:3] != '038'):\n",
        "                set_files.append(os.path.join(root, file))\n",
        "\n",
        "# 假設我們有相應的標籤文件或手動標籤\n",
        "labels = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0,0 ,0 ,0 ,1 ,1 ,1 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,0 ,1 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1]  # 替換為實際標籤\n",
        "\n",
        "# 檢查文件數量和標籤數量是否匹配\n",
        "print(len(set_files),len(labels))\n",
        "assert len(set_files) == len(labels), \"文件數量和標籤數量不匹配\"\n",
        "\n",
        "# 存儲特徵和標籤的列表\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# 存儲所有数据的列表\n",
        "all_data = []\n",
        "\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    all_data.append(raw.get_data())\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data in all_data])\n",
        "\n",
        "# Truncate or pad data to match the maximum data length\n",
        "for data, label in zip(all_data, labels):\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the max length, pad with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    else:\n",
        "        # If data length is greater than or equal to the max length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    # Add to feature list\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "# 將特徵和標籤轉換為NumPy數組\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# 分割訓練集和測試集\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=150)\n",
        "\n",
        "# 轉換為DMatrix格式，這是XGBoost的內部數據格式\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# 設置XGBoost參數\n",
        "params = {\n",
        "    'booster': 'gblinear',\n",
        "    'objective': 'binary:logistic',  # 二分類問題\n",
        "    'eval_metric': 'rmse',  # 評估指標\n",
        "    'eta': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'seed': 60\n",
        "}\n",
        "\n",
        "# 訓練模型\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# 進行預測\n",
        "y_pred_prob = bst.predict(dtest)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# 評估模型\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"準確率: {accuracy:.2f}\")\n",
        "\n",
        "# 顯示分類報告\n",
        "print(\"\\n分類報告:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# 繪製混淆矩陣\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o0ztw1k02q5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forest1"
      ],
      "metadata": {
        "id": "1FvyNIiH21tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'trans-01'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if file[0:3] != '052':\n",
        "                set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,\n",
        "0 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,1 ,0 ,0 ,1,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1,1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data\n",
        "all_data = []\n",
        "\n",
        "# Frequency bands\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    all_data.append(raw.get_data())\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data in all_data])\n",
        "\n",
        "# Truncate or pad data to match the maximum data length\n",
        "for data, label in zip(all_data, labels):\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the max length, pad with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    else:\n",
        "        # If data length is greater than or equal to the max length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    # Add to feature list\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and target to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=150)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SHVSksgR23v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forest2"
      ],
      "metadata": {
        "id": "i4lxjny92-DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import mne\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the folder path\n",
        "folder_path = r'trans-02'\n",
        "set_files = []\n",
        "# List all .set files in the folder\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.set'):\n",
        "            if file[0:3] != '038':\n",
        "                set_files.append(os.path.join(root, file))\n",
        "\n",
        "# Assuming we have corresponding label files or manual labels\n",
        "labels = [0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0,0 ,0 ,0 ,1 ,1 ,1 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,0 ,1 ,0 ,0 ,0 ,0 ,1 ,0 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1 ,1]  # Replace with actual labels\n",
        "\n",
        "# Check if the number of files and labels match\n",
        "print(len(set_files), len(labels))\n",
        "assert len(set_files) == len(labels), \"Number of files and labels do not match\"\n",
        "\n",
        "# Lists to store features and labels\n",
        "features = []\n",
        "target = []\n",
        "\n",
        "# List to store all data\n",
        "all_data = []\n",
        "\n",
        "# Frequency bands\n",
        "bands = {\n",
        "    'delta': (1, 4),\n",
        "    'theta': (4, 8),\n",
        "    'alpha': (8, 12),\n",
        "    'beta': (12, 30)\n",
        "}\n",
        "\n",
        "# Read and process each file\n",
        "for file, label in zip(set_files, labels):\n",
        "    print(file)\n",
        "    raw = mne.io.read_raw_eeglab(file, preload=True)\n",
        "    all_data.append(raw.get_data())\n",
        "\n",
        "# Find the maximum data length\n",
        "max_length = max([data.shape[1] for data in all_data])\n",
        "\n",
        "# Truncate or pad data to match the maximum data length\n",
        "for data, label in zip(all_data, labels):\n",
        "    if data.shape[1] < max_length:\n",
        "        # If data length is less than the max length, pad with zeros\n",
        "        padded_data = np.pad(data, ((0, 0), (0, max_length - data.shape[1])), mode='constant')\n",
        "    else:\n",
        "        # If data length is greater than or equal to the max length, truncate the data\n",
        "        padded_data = data[:, :max_length]\n",
        "\n",
        "    feature_vector = []\n",
        "\n",
        "    # Filter data for each band and extract features\n",
        "    for band, (low_freq, high_freq) in bands.items():\n",
        "        filtered_data = mne.filter.filter_data(padded_data, sfreq=raw.info['sfreq'], l_freq=low_freq, h_freq=high_freq)\n",
        "        mean_features = np.mean(filtered_data, axis=1)\n",
        "        std_features = np.std(filtered_data, axis=1)\n",
        "        ptp_features = np.ptp(filtered_data, axis=1)  # Peak-to-peak value\n",
        "        # Combine features for this band\n",
        "        feature_vector = np.concatenate([feature_vector, mean_features, std_features, ptp_features])\n",
        "\n",
        "    # Add to feature list\n",
        "    features.append(feature_vector)\n",
        "    target.append(label)\n",
        "\n",
        "# Convert features and target to NumPy arrays\n",
        "features = np.array(features)\n",
        "target = np.array(target)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=150)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Depressed']))\n",
        "\n",
        "# Plot confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Depressed'], yticklabels=['Normal', 'Depressed'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rfJ9a4bh3AUm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}